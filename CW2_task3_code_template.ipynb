{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Enhanced Multi-Label Movie Genre Classifier\n",
    "## NO FUNCTIONS - JUST PURE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please keep only necessary information in this cell.\n",
    "\n",
    "#----------------------Please keep all following constants unchanged.----------------------------------------\n",
    "NUM_ROWS_VALIDATION = 1031 # Number of rows in validation set\n",
    "NUM_ROWS_TEST = 1053 # Number of rows in test set\n",
    "\n",
    "#----------------------Please modify the following constants to fit your actual value.-----------------------\n",
    "STUDENT_ID = 'your_student_id'  # Replace with your actual 8-digits student ID\n",
    "TRAINING_SET = './data/CW2_training_dataset.csv' # Replace with the actual path to your training dataset csv file\n",
    "VALIDATION_SET = './data/CW2_validation_dataset.csv'  # Replace with the actual path to your validation dataset csv file\n",
    "VALIDATION_SET_OUTPUT = f'./data/{STUDENT_ID}_CW2_task3_validation_results.csv'  # Replace with the actual path to your validation prediction csv file\n",
    "TEST_SET_INPUT = './data/CW2_test_dataset.csv'  # Replace with the actual path to your test prediction csv file\n",
    "\n",
    "\n",
    "#----------------------Your constants------------------------------------------------\n",
    "# By adding more constants here, you can help improve the clarity and maintainability of your code and make the reviewing easier for TAs.\n",
    "MODEL_NAME = 'roberta-base'\n",
    "NUM_LABELS = 8\n",
    "MAX_LENGTH = 256\n",
    "CONCAT_LAST_N_LAYERS = 4\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "FOCAL_GAMMA = 2.0\n",
    "\n",
    "GENRE_COLS = ['comedy', 'cult', 'flashback', 'historical', 'revenge', 'romantic', 'scifi', 'violence']\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/iusers01/fse-ugpgt01/compsci01/y12806ja/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#---------------------Required imports----------------------\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import os.path\n",
    "import csv\n",
    "from sklearn.metrics import f1_score\n",
    "#----------------------Your imports-------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7127\n",
      "                                     ID                        title  \\\n",
      "0  ee7722b2-bc23-400b-9461-4ff91f01f486                  Next of Kin   \n",
      "1  3b111f7d-0c19-4cb3-84a1-d6dc687c9716              The Survivalist   \n",
      "2  3116352f-4b50-43a2-b9be-458c4aa086e5                  Superman II   \n",
      "3  bbb71d71-1503-4aa6-9129-918b9efc3c3f  The Hunchback of Notre Dame   \n",
      "4  c12f67ca-5825-43d0-b9a7-61c348915715                         Taxi   \n",
      "\n",
      "                                       plot_synopsis  comedy  cult  flashback  \\\n",
      "0  Truman Gates (Patrick Swayze), raised in Appal...       0     0          0   \n",
      "1  The film takes place when oil production has c...       0     1          0   \n",
      "2  Before the destruction of Krypton, the crimina...       0     0          0   \n",
      "3  The gypsy Esmeralda captures the hearts of man...       0     0          0   \n",
      "4  Taxi portrays director Jafar Panahi as he cour...       0     0          0   \n",
      "\n",
      "   historical  revenge  romantic  scifi  violence  \n",
      "0           0        1         0      0         1  \n",
      "1           0        0         0      0         0  \n",
      "2           0        1         0      0         1  \n",
      "3           0        0         1      0         0  \n",
      "4           0        0         0      0         1  \n",
      "comedy      :  1230 (17.3%)\n",
      "cult        :  1801 (25.3%)\n",
      "flashback   :  1995 (28.0%)\n",
      "historical  :   191 (2.7%)\n",
      "revenge     :  1680 (23.6%)\n",
      "romantic    :  1993 (28.0%)\n",
      "scifi       :   207 (2.9%)\n",
      "violence    :  3032 (42.5%)\n",
      "\n",
      "Train: 6057, Val: 1070\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAINING_SET)\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(train_df.head())\n",
    "\n",
    "for genre in GENRE_COLS:\n",
    "    count = train_df[genre].sum()\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"{genre:12s}: {count:5d} ({percentage:.1f}%)\")\n",
    "\n",
    "texts = (train_df['title'] + ' [SEP] ' + train_df['plot_synopsis']).values\n",
    "labels = train_df[GENRE_COLS].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.15, random_state=RANDOM_SEED)\n",
    "print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: roberta-base\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders created: 379 train batches, 67 val batches\n"
     ]
    }
   ],
   "source": [
    "class MovieGenreDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }\n",
    "\n",
    "train_dataset = MovieGenreDataset(X_train, y_train, tokenizer, MAX_LENGTH)\n",
    "val_dataset = MovieGenreDataset(X_val, y_val, tokenizer, MAX_LENGTH)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "print(f\"Dataloaders created: {len(train_loader)} train batches, {len(val_loader)} val batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 131,747,336\n",
      "Trainable params: 131,747,336\n",
      "Under 600M: True\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        return (self.alpha * (1-pt)**self.gamma * BCE_loss).mean()\n",
    "\n",
    "class MultiHeadAttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query = self.query.expand(hidden_states.size(0), -1, -1)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = ~attention_mask.bool()\n",
    "        attn_output, _ = self.attention(query, hidden_states, hidden_states, key_padding_mask=attention_mask)\n",
    "        return attn_output.squeeze(1)\n",
    "\n",
    "class EnhancedTransformerClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, concat_last_n_layers=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.transformer.config.hidden_size\n",
    "        self.concat_last_n_layers = concat_last_n_layers\n",
    "        feature_size = self.hidden_size * concat_last_n_layers\n",
    "        self.attention_pooling = MultiHeadAttentionPooling(self.hidden_size, num_heads=8)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(feature_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_size, feature_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(feature_size // 2, num_labels)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        last_n_layers = outputs.hidden_states[-self.concat_last_n_layers:]\n",
    "        pooled_layers = [self.attention_pooling(layer, attention_mask) for layer in last_n_layers]\n",
    "        features = torch.cat(pooled_layers, dim=-1)\n",
    "        features = self.layer_norm(self.dropout(features))\n",
    "        return self.classifier(features)\n",
    "\n",
    "model = EnhancedTransformerClassifier(MODEL_NAME, NUM_LABELS, CONCAT_LAST_N_LAYERS, DROPOUT)\n",
    "model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Trainable params: {trainable_params:,}\")\n",
    "print(f\"Under 600M: {trainable_params < 600_000_000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer and scheduler ready\n",
      "Training steps: 3790, Warmup: 379\n"
     ]
    }
   ],
   "source": [
    "criterion = FocalLoss(FOCAL_ALPHA, FOCAL_GAMMA)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_params = [\n",
    "    {'params': [p for n, p in model.transformer.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WEIGHT_DECAY, 'lr': LEARNING_RATE * 0.1},\n",
    "    {'params': [p for n, p in model.transformer.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, 'lr': LEARNING_RATE * 0.1},\n",
    "    {'params': [p for n, p in model.named_parameters() if 'transformer' not in n], 'weight_decay': WEIGHT_DECAY, 'lr': LEARNING_RATE}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_params)\n",
    "\n",
    "num_training_steps = len(train_loader) * NUM_EPOCHS\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_RATIO)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "print(f\"Optimizer and scheduler ready\")\n",
    "print(f\"Training steps: {num_training_steps}, Warmup: {num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [01:25<00:00,  4.45it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:06<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0270\n",
      "Val Loss: 0.0254\n",
      "Micro F1: 0.4673\n",
      "Macro F1: 0.2962\n",
      "Samples F1: 0.4205\n",
      "Hamming: 0.1777\n",
      "  comedy      : 0.1111\n",
      "  cult        : 0.4346\n",
      "  flashback   : 0.3571\n",
      "  historical  : 0.0000\n",
      "  revenge     : 0.0000\n",
      "  romantic    : 0.6039\n",
      "  scifi       : 0.1765\n",
      "  violence    : 0.6866\n",
      "✓ Best model updated (F1: 0.4673)\n",
      "\n",
      "Epoch 2/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [01:25<00:00,  4.42it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:06<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0253\n",
      "Val Loss: 0.0247\n",
      "Micro F1: 0.4690\n",
      "Macro F1: 0.3558\n",
      "Samples F1: 0.4109\n",
      "Hamming: 0.1703\n",
      "  comedy      : 0.2747\n",
      "  cult        : 0.4444\n",
      "  flashback   : 0.3417\n",
      "  historical  : 0.2581\n",
      "  revenge     : 0.0806\n",
      "  romantic    : 0.6075\n",
      "  scifi       : 0.1765\n",
      "  violence    : 0.6625\n",
      "✓ Best model updated (F1: 0.4690)\n",
      "\n",
      "Epoch 3/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [01:25<00:00,  4.44it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:06<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0243\n",
      "Val Loss: 0.0246\n",
      "Micro F1: 0.4833\n",
      "Macro F1: 0.3616\n",
      "Samples F1: 0.4224\n",
      "Hamming: 0.1703\n",
      "  comedy      : 0.3064\n",
      "  cult        : 0.2783\n",
      "  flashback   : 0.4755\n",
      "  historical  : 0.1429\n",
      "  revenge     : 0.2761\n",
      "  romantic    : 0.5511\n",
      "  scifi       : 0.1765\n",
      "  violence    : 0.6864\n",
      "✓ Best model updated (F1: 0.4833)\n",
      "\n",
      "Epoch 4/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [01:26<00:00,  4.40it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:06<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0235\n",
      "Val Loss: 0.0245\n",
      "Micro F1: 0.5055\n",
      "Macro F1: 0.3918\n",
      "Samples F1: 0.4387\n",
      "Hamming: 0.1666\n",
      "  comedy      : 0.3009\n",
      "  cult        : 0.4860\n",
      "  flashback   : 0.4575\n",
      "  historical  : 0.1379\n",
      "  revenge     : 0.2420\n",
      "  romantic    : 0.4977\n",
      "  scifi       : 0.3077\n",
      "  violence    : 0.7046\n",
      "✓ Best model updated (F1: 0.5055)\n",
      "\n",
      "Epoch 5/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [01:25<00:00,  4.46it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:06<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0227\n",
      "Val Loss: 0.0244\n",
      "Micro F1: 0.5336\n",
      "Macro F1: 0.4552\n",
      "Samples F1: 0.4790\n",
      "Hamming: 0.1671\n",
      "  comedy      : 0.3500\n",
      "  cult        : 0.4722\n",
      "  flashback   : 0.4600\n",
      "  historical  : 0.4186\n",
      "  revenge     : 0.3949\n",
      "  romantic    : 0.5691\n",
      "  scifi       : 0.2703\n",
      "  violence    : 0.7064\n",
      "✓ Best model updated (F1: 0.5336)\n",
      "\n",
      "Epoch 6/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [01:25<00:00,  4.44it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:06<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0222\n",
      "Val Loss: 0.0245\n",
      "Micro F1: 0.5407\n",
      "Macro F1: 0.4631\n",
      "Samples F1: 0.4845\n",
      "Hamming: 0.1636\n",
      "  comedy      : 0.3291\n",
      "  cult        : 0.5107\n",
      "  flashback   : 0.4551\n",
      "  historical  : 0.4000\n",
      "  revenge     : 0.4020\n",
      "  romantic    : 0.5862\n",
      "  scifi       : 0.3158\n",
      "  violence    : 0.7061\n",
      "✓ Best model updated (F1: 0.5407)\n",
      "\n",
      "Epoch 7/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [01:24<00:00,  4.47it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:06<00:00, 10.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0217\n",
      "Val Loss: 0.0248\n",
      "Micro F1: 0.5451\n",
      "Macro F1: 0.4688\n",
      "Samples F1: 0.4979\n",
      "Hamming: 0.1675\n",
      "  comedy      : 0.3792\n",
      "  cult        : 0.5198\n",
      "  flashback   : 0.4612\n",
      "  historical  : 0.3889\n",
      "  revenge     : 0.4041\n",
      "  romantic    : 0.5851\n",
      "  scifi       : 0.3077\n",
      "  violence    : 0.7042\n",
      "✓ Best model updated (F1: 0.5451)\n",
      "\n",
      "Epoch 8/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [01:16<00:00,  4.93it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:04<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0215\n",
      "Val Loss: 0.0250\n",
      "Micro F1: 0.5448\n",
      "Macro F1: 0.4705\n",
      "Samples F1: 0.4928\n",
      "Hamming: 0.1655\n",
      "  comedy      : 0.3817\n",
      "  cult        : 0.5096\n",
      "  flashback   : 0.4534\n",
      "  historical  : 0.3889\n",
      "  revenge     : 0.4051\n",
      "  romantic    : 0.5709\n",
      "  scifi       : 0.3415\n",
      "  violence    : 0.7128\n",
      "\n",
      "Epoch 9/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [00:53<00:00,  7.11it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:04<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0213\n",
      "Val Loss: 0.0250\n",
      "Micro F1: 0.5410\n",
      "Macro F1: 0.4682\n",
      "Samples F1: 0.4908\n",
      "Hamming: 0.1659\n",
      "  comedy      : 0.3707\n",
      "  cult        : 0.4879\n",
      "  flashback   : 0.4549\n",
      "  historical  : 0.3889\n",
      "  revenge     : 0.4092\n",
      "  romantic    : 0.5764\n",
      "  scifi       : 0.3500\n",
      "  violence    : 0.7075\n",
      "\n",
      "Epoch 10/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 379/379 [00:53<00:00,  7.11it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:04<00:00, 15.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0213\n",
      "Val Loss: 0.0250\n",
      "Micro F1: 0.5445\n",
      "Macro F1: 0.4699\n",
      "Samples F1: 0.4952\n",
      "Hamming: 0.1655\n",
      "  comedy      : 0.3672\n",
      "  cult        : 0.5054\n",
      "  flashback   : 0.4606\n",
      "  historical  : 0.3889\n",
      "  revenge     : 0.4051\n",
      "  romantic    : 0.5840\n",
      "  scifi       : 0.3415\n",
      "  violence    : 0.7069\n",
      "\n",
      "Training done! Best F1: 0.5451\n",
      "Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "best_model_state = None\n",
    "best_val_preds = None\n",
    "best_val_labels = None\n",
    "history = {'train_loss': [], 'val_loss': [], 'micro_f1': [], 'macro_f1': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # TRAINING\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_preds_list = []\n",
    "    train_labels_list = []\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        preds = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        train_preds_list.append(preds)\n",
    "        train_labels_list.append(labels.cpu().numpy())\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_preds = np.vstack(train_preds_list)\n",
    "    train_labels = np.vstack(train_labels_list)\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_preds_list = []\n",
    "    val_labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.sigmoid(logits).cpu().numpy()\n",
    "            val_preds_list.append(preds)\n",
    "            val_labels_list.append(labels.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_preds = np.vstack(val_preds_list)\n",
    "    val_labels = np.vstack(val_labels_list)\n",
    "    \n",
    "    # METRICS\n",
    "    val_preds_binary = (val_preds > 0.5).astype(int)\n",
    "    micro_f1 = f1_score(val_labels, val_preds_binary, average='micro')\n",
    "    macro_f1 = f1_score(val_labels, val_preds_binary, average='macro')\n",
    "    samples_f1 = f1_score(val_labels, val_preds_binary, average='samples')\n",
    "    hamming = hamming_loss(val_labels, val_preds_binary)\n",
    "    \n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['micro_f1'].append(micro_f1)\n",
    "    history['macro_f1'].append(macro_f1)\n",
    "    \n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Micro F1: {micro_f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Samples F1: {samples_f1:.4f}\")\n",
    "    print(f\"Hamming: {hamming:.4f}\")\n",
    "    \n",
    "    per_class_f1 = f1_score(val_labels, val_preds_binary, average=None)\n",
    "    for i, genre in enumerate(GENRE_COLS):\n",
    "        print(f\"  {genre:12s}: {per_class_f1[i]:.4f}\")\n",
    "    \n",
    "    if micro_f1 > best_f1:\n",
    "        best_f1 = micro_f1\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        best_val_preds = val_preds.copy()\n",
    "        best_val_labels = val_labels.copy()\n",
    "        print(f\"✓ Best model updated (F1: {best_f1:.4f})\")\n",
    "\n",
    "print(f\"\\nTraining done! Best F1: {best_f1:.4f}\")\n",
    "\n",
    "# Load best model state back into the model\n",
    "model.load_state_dict(best_model_state)\n",
    "print(\"Best model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Best Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting predictions: 100%|██████████| 67/67 [00:04<00:00, 15.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs shape: (1070, 8)\n",
      "Labels shape: (1070, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "val_preds_list = []\n",
    "val_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc='Getting predictions'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        \n",
    "        val_preds_list.append(probs)\n",
    "        val_labels_list.append(labels.numpy())\n",
    "\n",
    "probs_val = np.vstack(val_preds_list)\n",
    "y_val = np.vstack(val_labels_list)\n",
    "\n",
    "print(f\"Probs shape: {probs_val.shape}\")\n",
    "print(f\"Labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal thresholds...\n",
      "\n",
      "comedy      : threshold = 0.38, F1 = 0.4630\n",
      "cult        : threshold = 0.41, F1 = 0.5556\n",
      "flashback   : threshold = 0.37, F1 = 0.5450\n",
      "historical  : threshold = 0.46, F1 = 0.4444\n",
      "revenge     : threshold = 0.42, F1 = 0.5131\n",
      "romantic    : threshold = 0.40, F1 = 0.6386\n",
      "scifi       : threshold = 0.33, F1 = 0.4416\n",
      "violence    : threshold = 0.49, F1 = 0.7111\n",
      "\n",
      "BEST THRESHOLDS:\n",
      "[0.38 0.41 0.37 0.46 0.42 0.4  0.33 0.49]\n"
     ]
    }
   ],
   "source": [
    "# Find best thresholds - EXACTLY like your Task 1 code\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "best_thresholds = []\n",
    "\n",
    "print(\"Finding optimal thresholds...\\n\")\n",
    "\n",
    "for col in range(probs_val.shape[1]):\n",
    "    best_f1 = 0\n",
    "    best_thr = 0.5\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        preds = (probs_val[:, col] >= thr).astype(int)\n",
    "        f1 = f1_score(y_val[:, col], preds, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "    \n",
    "    best_thresholds.append(best_thr)\n",
    "    print(f\"{GENRE_COLS[col]:12s}: threshold = {best_thr:.2f}, F1 = {best_f1:.4f}\")\n",
    "\n",
    "best_thresholds = np.array(best_thresholds)\n",
    "print(f\"\\nBEST THRESHOLDS:\")\n",
    "print(best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default (0.5): Micro F1 = 0.5445, Macro F1 = 0.4699\n",
      "Optimized: Micro F1 = 0.5836, Macro F1 = 0.5390\n",
      "Improvement: +0.0391\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with optimized thresholds\n",
    "val_preds_optimized = np.zeros_like(probs_val)\n",
    "for i, threshold in enumerate(best_thresholds):\n",
    "    val_preds_optimized[:, i] = (probs_val[:, i] >= threshold).astype(int)\n",
    "\n",
    "micro_f1_opt = f1_score(y_val, val_preds_optimized, average='micro')\n",
    "macro_f1_opt = f1_score(y_val, val_preds_optimized, average='macro')\n",
    "samples_f1_opt = f1_score(y_val, val_preds_optimized, average='samples')\n",
    "\n",
    "val_preds_default = (probs_val > 0.5).astype(int)\n",
    "micro_f1_default = f1_score(y_val, val_preds_default, average='micro')\n",
    "macro_f1_default = f1_score(y_val, val_preds_default, average='macro')\n",
    "\n",
    "print(f\"\\nDefault (0.5): Micro F1 = {micro_f1_default:.4f}, Macro F1 = {macro_f1_default:.4f}\")\n",
    "print(f\"Optimized: Micro F1 = {micro_f1_opt:.4f}, Macro F1 = {macro_f1_opt:.4f}\")\n",
    "print(f\"Improvement: {micro_f1_opt - micro_f1_default:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dataframe(df, model, tokenizer, batch_size=BATCH_SIZE, device=\"cuda\"):\n",
    "    # Prepare texts\n",
    "    texts = (df['title'] + ' [SEP] ' + df['plot_synopsis']).values\n",
    "    dummy_labels = np.zeros((len(texts), NUM_LABELS))\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    dataset = MovieGenreDataset(texts, dummy_labels, tokenizer, MAX_LENGTH)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            # Apply per-label thresholds\n",
    "            thresholds_tensor = torch.tensor(best_thresholds, device=probs.device, dtype=probs.dtype)\n",
    "            preds = (probs >= thresholds_tensor).int()\n",
    "            \n",
    "            all_preds.append(preds.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    return all_preds  # shape: (len(df), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of your code cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(submission_file_path, gold_standard_file_path):\n",
    "    \"\"\"\n",
    "    Read submission and gold standard files.\n",
    "    Extract student ID from filename.\n",
    "    \"\"\"\n",
    "    # Try to find student ID from the filename (looks for 8 digit numbers)\n",
    "    id_regex = r'\\d{8}'\n",
    "\n",
    "    user_id = re.findall(id_regex, submission_file_path)\n",
    "    print(\"Found your ID: \", user_id)\n",
    "    if user_id:\n",
    "        user_id = user_id[0]\n",
    "    else:\n",
    "        user_id = 'Unknown'\n",
    "\n",
    "    # Load submission CSV\n",
    "    print(f\"\\nLoading submission file: {submission_file_path}\")\n",
    "    submission_df = pd.read_csv(submission_file_path, sep=',', header=None,\n",
    "                                quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "    # Load gold standard CSV\n",
    "    print(f\"Loading gold standard file: {gold_standard_file_path}\")\n",
    "    gold_standard_df = pd.read_csv(gold_standard_file_path, header=None)\n",
    "\n",
    "    # Remove columns 1 and 2 (keep only ID and labels)\n",
    "    gold_standard_df = gold_standard_df.drop([1, 2], axis=1)\n",
    "    # Skip header row\n",
    "    gold_standard_df = gold_standard_df.iloc[1:]\n",
    "\n",
    "    return submission_df, gold_standard_df, user_id\n",
    "\n",
    "\n",
    "def match_and_prepare_data(submission_df, gold_standard_df, user_id):\n",
    "    \"\"\"\n",
    "    Match submission rows with gold standard rows by ID.\n",
    "    Prepare data for evaluation.\n",
    "    \"\"\"\n",
    "    gold_standard_labels = []\n",
    "    submission_labels = []\n",
    "    missed_rows = []\n",
    "    submission_df_copy = submission_df.copy()\n",
    "\n",
    "    print(f\"\\nMatching submission with gold standard...\")\n",
    "    print(f\"Gold standard rows: {len(gold_standard_df)}\")\n",
    "    print(f\"Submission rows: {len(submission_df_copy)}\")\n",
    "\n",
    "    # Match each gold standard row with submission\n",
    "    for index, row in gold_standard_df.iterrows():\n",
    "        row = row.reset_index(drop=True)\n",
    "        row_found = False\n",
    "        row_id = row[0]\n",
    "\n",
    "        # Extract gold standard labels\n",
    "        row_labels = [int(row[i]) for i in range(1, len(row))]\n",
    "        gold_standard_labels.append(row_labels)\n",
    "\n",
    "        # Find corresponding submission row\n",
    "        for sub_index, submission_row in submission_df_copy.iterrows():\n",
    "            if submission_row[0].strip() == row_id.strip():\n",
    "                try:\n",
    "                    # Extract submission labels\n",
    "                    submission_row_labels = [int(submission_row[i]) for i in range(1, len(submission_row))]\n",
    "                except:\n",
    "                    # Handle malformed labels (take first character if multi-digit)\n",
    "                    submission_row_labels = [int(str(submission_row[i])[0]) for i in range(1, len(submission_row))]\n",
    "\n",
    "                submission_labels.append(submission_row_labels)\n",
    "                row_found = True\n",
    "                submission_df_copy.drop(sub_index, inplace=True)\n",
    "                break\n",
    "\n",
    "        if not row_found:\n",
    "            # If row is missing, add inverse labels (worst possible prediction)\n",
    "            missed_rows.append(row_id)\n",
    "            submission_labels.append([0 if label == 1 else 1 for label in row_labels])\n",
    "\n",
    "    return gold_standard_labels, submission_labels, missed_rows\n",
    "\n",
    "\n",
    "def evaluate_submission(gold_standard_labels, submission_labels):\n",
    "    \"\"\"\n",
    "    Calculate weighted F1 score.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCalculating weighted F1 score...\")\n",
    "\n",
    "    # Calculate weighted F1 score (accounts for class imbalance)\n",
    "    f1_weighted = f1_score(gold_standard_labels, submission_labels, average='weighted')\n",
    "\n",
    "    return f1_weighted\n",
    "\n",
    "\n",
    "def print_results(user_id, f1_weighted, missed_rows):\n",
    "    \"\"\"\n",
    "    Print evaluation results to screen.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"YOUR SUBMISSION EVALUATION REPORT\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Alert if ID not found in filename\n",
    "    if user_id == 'Unknown':\n",
    "        print('WARNING: ID not found in filename!')\n",
    "        print('   Please ensure your filename contains your 8-digit student ID.')\n",
    "        print()\n",
    "\n",
    "    print(f\"Your ID: {user_id}\")\n",
    "    print()\n",
    "\n",
    "    # Display F1 score with visual indicator\n",
    "    print(\"EVALUATION RESULTS:\")\n",
    "    print(f\"   Weighted F1 Score: {f1_weighted:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Report missing rows\n",
    "    if missed_rows:\n",
    "        print(f\"MISSING DATA ({len(missed_rows)} rows not found):\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, row in enumerate(missed_rows[:10], 1):  # Show first 10\n",
    "            print(f\"    {i}. Row ID: {row}\")\n",
    "        if len(missed_rows) > 10:\n",
    "            print(f\"    ... and {len(missed_rows) - 10} more missing rows\")\n",
    "        print()\n",
    "        print(\"TIP: Make sure your submission includes all required rows.\")\n",
    "        print(\"        Missing rows are penalized with worst possible predictions.\")\n",
    "    else:\n",
    "        print(\"DATA COMPLETENESS: All expected rows found in your submission!\")\n",
    "\n",
    "    print()\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "\n",
    "\n",
    "def evaluate(submission_path, gold_standard_path):\n",
    "    \"\"\"\n",
    "    Main function to run the submission evaluation script.\n",
    "    \"\"\"\n",
    "\n",
    "    submission_file = submission_path\n",
    "    gold_standard_file = gold_standard_path\n",
    "\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(submission_file):\n",
    "        print(f\"Error: Your submission file '{submission_file}' not found!\")\n",
    "        print(\"Make sure the file path is correct and the file exists.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not os.path.exists(gold_standard_file):\n",
    "        print(f\"Error: Gold standard file '{gold_standard_file}' not found!\")\n",
    "        print(\"Make sure you have the correct gold standard file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Read data\n",
    "        submission_df, gold_standard_df, user_id = read_data(submission_file, gold_standard_file)\n",
    "\n",
    "        # Step 2: Match and prepare data\n",
    "        gold_standard_labels, submission_labels, missed_rows = match_and_prepare_data(\n",
    "            submission_df, gold_standard_df, user_id\n",
    "        )\n",
    "\n",
    "        # Step 3: Evaluate\n",
    "        f1_weighted = evaluate_submission(gold_standard_labels, submission_labels)\n",
    "\n",
    "        # Step 4: Print results\n",
    "        print_results(user_id, f1_weighted, missed_rows)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {str(e)}\")\n",
    "        print(\"Please check that your files are in the correct CSV format.\")\n",
    "        print(\"Each row should contain: ID, label1, label2, label3, ...\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found your ID:  []\n",
      "\n",
      "Loading submission file: ./data/your_student_id_CW2_task3_validation_results.csv\n",
      "Loading gold standard file: ./data/CW2_validation_dataset.csv\n",
      "\n",
      "Matching submission with gold standard...\n",
      "Gold standard rows: 1031\n",
      "Submission rows: 1031\n",
      "\n",
      "Calculating weighted F1 score...\n",
      "\n",
      "======================================================================\n",
      "YOUR SUBMISSION EVALUATION REPORT\n",
      "======================================================================\n",
      "WARNING: ID not found in filename!\n",
      "   Please ensure your filename contains your 8-digit student ID.\n",
      "\n",
      "Your ID: Unknown\n",
      "\n",
      "EVALUATION RESULTS:\n",
      "   Weighted F1 Score: 0.5737\n",
      "\n",
      "DATA COMPLETENESS: All expected rows found in your submission!\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please run the evaluation scripts cell above before running the mark_and_record\n",
    "\n",
    "# Please make sure that output format is like following (no header row, no tilte and plot columns):\n",
    "# 94834c61-0e30-4799-9998-6f74f6sbb204\t0\t1\t0\t0\t1\t0\t0\t0\n",
    "# 559sdd28-b6a2-4662-ab55-a6678as26a56\t0\t0\t0\t0\t0\t0\t1\t0\n",
    "# b71y3317-04cd-42f5-a380-d21dfasdbd36\t0\t0\t0\t0\t1\t0\t0\t0\n",
    "\n",
    "evaluation_results = evaluate(VALIDATION_SET_OUTPUT, VALIDATION_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions to formatted file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now please modify the code to format your output csv file.\n",
    "\n",
    "# Please make sure that output format is like following (no header row, no tilte and plot columns):\n",
    "# 94834c61-0e30-4799-9998-6f74f6sbb204\t0\t1\t0\t0\t1\t0\t0\t0\n",
    "# 559sdd28-b6a2-4662-ab55-a6678as26a56\t0\t0\t0\t0\t0\t0\t1\t0\n",
    "# b71y3317-04cd-42f5-a380-d21dfasdbd36\t0\t0\t0\t0\t1\t0\t0\t0\n",
    "\n",
    "test_df = pd.read_csv(TEST_SET_INPUT)\n",
    "test_preds = predict_dataframe(test_df, model, tokenizer)\n",
    "\n",
    "output_df = pd.DataFrame(test_preds, columns=GENRE_COLS)\n",
    "output_df.insert(0, 'ID', test_df['ID'])\n",
    "\n",
    "# For example, if you have a DataFrame named 'output_df', you can save it\n",
    "assert isinstance(output_df, pd.DataFrame)\n",
    "assert len(output_df) == NUM_ROWS_TEST, \"Output length is not aligned with the testdata.csv.\"\n",
    "assert len(output_df.columns) == 9, \"Please make sure to follow the format above and keep only IDs and 8 columns of prediction.\"\n",
    "output_df.to_csv(f'./data/{STUDENT_ID}_CW2_task3_results.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
