{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Enhanced Multi-Label Movie Genre Classifier\n",
    "## NO FUNCTIONS - JUST PURE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "NUM_LABELS = 8\n",
    "MAX_LENGTH = 256\n",
    "CONCAT_LAST_N_LAYERS = 4\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "FOCAL_GAMMA = 2.0\n",
    "\n",
    "TRAIN_PATH = 'train.csv'\n",
    "TEST_PATH = 'test.csv'\n",
    "MODEL_SAVE_PATH = 'best_model_task3.pt'\n",
    "\n",
    "GENRE_NAMES = ['comedy', 'cult', 'flashback', 'historical', 'revenge', 'romantic', 'scifi', 'violence']\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(train_df.head())\n",
    "\n",
    "for genre in GENRE_NAMES:\n",
    "    count = train_df[genre].sum()\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"{genre:12s}: {count:5d} ({percentage:.1f}%)\")\n",
    "\n",
    "texts = (train_df['title'] + ' [SEP] ' + train_df['plot']).values\n",
    "labels = train_df[GENRE_NAMES].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.15, random_state=RANDOM_SEED)\n",
    "print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieGenreDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }\n",
    "\n",
    "train_dataset = MovieGenreDataset(X_train, y_train, tokenizer, MAX_LENGTH)\n",
    "val_dataset = MovieGenreDataset(X_val, y_val, tokenizer, MAX_LENGTH)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "print(f\"Dataloaders created: {len(train_loader)} train batches, {len(val_loader)} val batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        return (self.alpha * (1-pt)**self.gamma * BCE_loss).mean()\n",
    "\n",
    "class MultiHeadAttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query = self.query.expand(hidden_states.size(0), -1, -1)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = ~attention_mask.bool()\n",
    "        attn_output, _ = self.attention(query, hidden_states, hidden_states, key_padding_mask=attention_mask)\n",
    "        return attn_output.squeeze(1)\n",
    "\n",
    "class EnhancedTransformerClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, concat_last_n_layers=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.transformer.config.hidden_size\n",
    "        self.concat_last_n_layers = concat_last_n_layers\n",
    "        feature_size = self.hidden_size * concat_last_n_layers\n",
    "        self.attention_pooling = MultiHeadAttentionPooling(self.hidden_size, num_heads=8)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(feature_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_size, feature_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(feature_size // 2, num_labels)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        last_n_layers = outputs.hidden_states[-self.concat_last_n_layers:]\n",
    "        pooled_layers = [self.attention_pooling(layer, attention_mask) for layer in last_n_layers]\n",
    "        features = torch.cat(pooled_layers, dim=-1)\n",
    "        features = self.layer_norm(self.dropout(features))\n",
    "        return self.classifier(features)\n",
    "\n",
    "model = EnhancedTransformerClassifier(MODEL_NAME, NUM_LABELS, CONCAT_LAST_N_LAYERS, DROPOUT)\n",
    "model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Trainable params: {trainable_params:,}\")\n",
    "print(f\"Under 600M: {trainable_params < 600_000_000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(FOCAL_ALPHA, FOCAL_GAMMA)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_params = [\n",
    "    {'params': [p for n, p in model.transformer.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WEIGHT_DECAY, 'lr': LEARNING_RATE * 0.1},\n",
    "    {'params': [p for n, p in model.transformer.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, 'lr': LEARNING_RATE * 0.1},\n",
    "    {'params': [p for n, p in model.named_parameters() if 'transformer' not in n], 'weight_decay': WEIGHT_DECAY, 'lr': LEARNING_RATE}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_params)\n",
    "\n",
    "num_training_steps = len(train_loader) * NUM_EPOCHS\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_RATIO)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "print(f\"Optimizer and scheduler ready\")\n",
    "print(f\"Training steps: {num_training_steps}, Warmup: {num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'micro_f1': [], 'macro_f1': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # TRAINING\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_preds_list = []\n",
    "    train_labels_list = []\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        preds = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        train_preds_list.append(preds)\n",
    "        train_labels_list.append(labels.cpu().numpy())\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_preds = np.vstack(train_preds_list)\n",
    "    train_labels = np.vstack(train_labels_list)\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_preds_list = []\n",
    "    val_labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.sigmoid(logits).cpu().numpy()\n",
    "            val_preds_list.append(preds)\n",
    "            val_labels_list.append(labels.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_preds = np.vstack(val_preds_list)\n",
    "    val_labels = np.vstack(val_labels_list)\n",
    "    \n",
    "    # METRICS\n",
    "    val_preds_binary = (val_preds > 0.5).astype(int)\n",
    "    micro_f1 = f1_score(val_labels, val_preds_binary, average='micro')\n",
    "    macro_f1 = f1_score(val_labels, val_preds_binary, average='macro')\n",
    "    samples_f1 = f1_score(val_labels, val_preds_binary, average='samples')\n",
    "    hamming = hamming_loss(val_labels, val_preds_binary)\n",
    "    \n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['micro_f1'].append(micro_f1)\n",
    "    history['macro_f1'].append(macro_f1)\n",
    "    \n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Micro F1: {micro_f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Samples F1: {samples_f1:.4f}\")\n",
    "    print(f\"Hamming: {hamming:.4f}\")\n",
    "    \n",
    "    per_class_f1 = f1_score(val_labels, val_preds_binary, average=None)\n",
    "    for i, genre in enumerate(GENRE_NAMES):\n",
    "        print(f\"  {genre:12s}: {per_class_f1[i]:.4f}\")\n",
    "    \n",
    "    if micro_f1 > best_f1:\n",
    "        best_f1 = micro_f1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'micro_f1': micro_f1,\n",
    "            'val_preds': val_preds,\n",
    "            'val_labels': val_labels\n",
    "        }, MODEL_SAVE_PATH)\n",
    "        print(f\"âœ“ Saved best model (F1: {best_f1:.4f})\")\n",
    "\n",
    "print(f\"\\nTraining done! Best F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Best Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "val_preds_list = []\n",
    "val_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc='Getting predictions'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        \n",
    "        val_preds_list.append(probs)\n",
    "        val_labels_list.append(labels.numpy())\n",
    "\n",
    "probs_val = np.vstack(val_preds_list)\n",
    "y_val = np.vstack(val_labels_list)\n",
    "\n",
    "print(f\"Probs shape: {probs_val.shape}\")\n",
    "print(f\"Labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best thresholds - EXACTLY like your Task 1 code\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "best_thresholds = []\n",
    "\n",
    "print(\"Finding optimal thresholds...\\n\")\n",
    "\n",
    "for col in range(probs_val.shape[1]):\n",
    "    best_f1 = 0\n",
    "    best_thr = 0.5\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        preds = (probs_val[:, col] >= thr).astype(int)\n",
    "        f1 = f1_score(y_val[:, col], preds, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "    \n",
    "    best_thresholds.append(best_thr)\n",
    "    print(f\"{GENRE_NAMES[col]:12s}: threshold = {best_thr:.2f}, F1 = {best_f1:.4f}\")\n",
    "\n",
    "best_thresholds = np.array(best_thresholds)\n",
    "print(f\"\\nBEST THRESHOLDS:\")\n",
    "print(best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with optimized thresholds\n",
    "val_preds_optimized = np.zeros_like(probs_val)\n",
    "for i, threshold in enumerate(best_thresholds):\n",
    "    val_preds_optimized[:, i] = (probs_val[:, i] >= threshold).astype(int)\n",
    "\n",
    "micro_f1_opt = f1_score(y_val, val_preds_optimized, average='micro')\n",
    "macro_f1_opt = f1_score(y_val, val_preds_optimized, average='macro')\n",
    "samples_f1_opt = f1_score(y_val, val_preds_optimized, average='samples')\n",
    "\n",
    "val_preds_default = (probs_val > 0.5).astype(int)\n",
    "micro_f1_default = f1_score(y_val, val_preds_default, average='micro')\n",
    "macro_f1_default = f1_score(y_val, val_preds_default, average='macro')\n",
    "\n",
    "print(f\"\\nDefault (0.5): Micro F1 = {micro_f1_default:.4f}, Macro F1 = {macro_f1_default:.4f}\")\n",
    "print(f\"Optimized: Micro F1 = {micro_f1_opt:.4f}, Macro F1 = {macro_f1_opt:.4f}\")\n",
    "print(f\"Improvement: {micro_f1_opt - micro_f1_default:+.4f}\")\n",
    "\n",
    "checkpoint['best_thresholds'] = best_thresholds\n",
    "torch.save(checkpoint, MODEL_SAVE_PATH)\n",
    "print(f\"Thresholds saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Prepare test texts\n",
    "test_texts = (test_df['title'] + ' [SEP] ' + test_df['plot']).values\n",
    "test_dummy_labels = np.zeros((len(test_texts), NUM_LABELS))\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_dataset = MovieGenreDataset(test_texts, test_dummy_labels, tokenizer, MAX_LENGTH)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Test loader ready: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "model.eval()\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Predicting'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "\n",
    "test_probabilities = np.vstack(all_probs)\n",
    "print(f\"Probabilities shape: {test_probabilities.shape}\")\n",
    "\n",
    "# Apply thresholds\n",
    "test_predictions = np.zeros_like(test_probabilities)\n",
    "for i, threshold in enumerate(best_thresholds):\n",
    "    test_predictions[:, i] = (test_probabilities[:, i] >= threshold).astype(int)\n",
    "\n",
    "print(f\"Predictions shape: {test_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission_df = pd.DataFrame(test_predictions, columns=GENRE_NAMES)\n",
    "if 'id' in test_df.columns:\n",
    "    submission_df.insert(0, 'id', test_df['id'].values)\n",
    "\n",
    "submission_df.to_csv('submission_task3.csv', index=False)\n",
    "print(\"Submission saved: submission_task3.csv\")\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "print(\"\\nPrediction statistics:\")\n",
    "for i, genre in enumerate(GENRE_NAMES):\n",
    "    count = test_predictions[:, i].sum()\n",
    "    percentage = (count / len(test_predictions)) * 100\n",
    "    avg_prob = test_probabilities[:, i].mean()\n",
    "    print(f\"{genre:12s}: {count:5d} ({percentage:5.1f}%) | Avg prob: {avg_prob:.3f}\")\n",
    "\n",
    "genres_per_movie = test_predictions.sum(axis=1)\n",
    "print(f\"\\nGenres per movie: Min={genres_per_movie.min()}, Max={genres_per_movie.max()}, Mean={genres_per_movie.mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
